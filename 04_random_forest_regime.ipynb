{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 â€” Random Forest + Regime Analysis\n",
    "\n",
    "**Goal:** Upgrade from Logistic Regression to Random Forest.\n",
    "Key additions:\n",
    "- Full feature set (all technical + fundamental + macro signals)\n",
    "- Feature importance chart â€” does `rate_regime` show up as significant?\n",
    "- SHAP values â€” *why* is the model predicting what it predicts per event?\n",
    "- Regime interaction test â€” do different features matter in different rate environments?\n",
    "- 3-class target (Buy / Hold / Sell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import lightgbm as lgb\n",
    "import shap\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, roc_auc_score, log_loss,\n",
    "    classification_report, ConfusionMatrixDisplay\n",
    ")\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "panel = pd.read_csv('../data/panel_dataset.csv', parse_dates=['earn_date'])\n",
    "panel = panel.sort_values('earn_date').reset_index(drop=True)\n",
    "print(f'Panel loaded: {panel.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Feature set: all signals â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "EXCLUDE = ['ticker', 'earn_date', 'sector',\n",
    "           'target_binary', 'target_mag',\n",
    "           'stock_ret_5d', 'spx_ret_5d', 'excess_ret_5d',\n",
    "           'sue_quintile', 'vol_high']  # derived/target cols\n",
    "\n",
    "# Encode sector as numeric\n",
    "le = LabelEncoder()\n",
    "panel['sector_enc'] = le.fit_transform(panel['sector'].fillna('Unknown'))\n",
    "\n",
    "ALL_FEATURES = [c for c in panel.columns\n",
    "                if c not in EXCLUDE and panel[c].dtype in ['float64', 'int64']]\n",
    "\n",
    "print(f'Total features: {len(ALL_FEATURES)}')\n",
    "print(ALL_FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Prepare data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "df_model = panel[ALL_FEATURES + ['target_binary', 'target_mag',\n",
    "                                   'earn_date', 'excess_ret_5d',\n",
    "                                   'ticker', 'rate_regime']].copy()\n",
    "df_model = df_model.dropna(subset=['target_binary', 'sue']).reset_index(drop=True)\n",
    "\n",
    "X = df_model[ALL_FEATURES].values\n",
    "y_bin = df_model['target_binary'].values\n",
    "y_mag = df_model['target_mag'].values\n",
    "\n",
    "# Impute missing values\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X = imputer.fit_transform(X)\n",
    "\n",
    "print(f'Dataset: {X.shape[0]} events Ã— {X.shape[1]} features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ LightGBM: Binary Classification â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# LightGBM > Random Forest for tabular data: handles missing values,\n",
    "# learns feature interactions, faster, less prone to overfitting\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5, gap=5)\n",
    "\n",
    "lgbm_params = dict(\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.03,\n",
    "    max_depth=5,\n",
    "    num_leaves=31,\n",
    "    min_child_samples=25,   # prevents overfitting on small leaves\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.7,\n",
    "    reg_alpha=0.1,          # L1 regularisation\n",
    "    reg_lambda=0.1,         # L2 regularisation\n",
    "    class_weight='balanced',\n",
    "    random_state=42,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "fold_results = []\n",
    "all_proba    = np.zeros(len(y_bin))\n",
    "final_model  = None\n",
    "\n",
    "print('Training LightGBM (Binary) with TimeSeriesSplit...\\n')\n",
    "print(f'  {\"Fold\":<6} {\"Accuracy\":<12} {\"AUC\":<10} {\"LogLoss\"}')\n",
    "print('  ' + '-'*40)\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(tscv.split(X)):\n",
    "    X_tr, X_te = X[train_idx], X[test_idx]\n",
    "    y_tr, y_te = y_bin[train_idx], y_bin[test_idx]\n",
    "\n",
    "    model = lgb.LGBMClassifier(**lgbm_params)\n",
    "    model.fit(\n",
    "        X_tr, y_tr,\n",
    "        eval_set=[(X_te, y_te)],\n",
    "        callbacks=[lgb.early_stopping(40, verbose=False),\n",
    "                   lgb.log_evaluation(-1)]\n",
    "    )\n",
    "\n",
    "    proba = model.predict_proba(X_te)[:, 1]\n",
    "    all_proba[test_idx] = proba\n",
    "\n",
    "    acc = accuracy_score(y_te, (proba > 0.5).astype(int))\n",
    "    auc = roc_auc_score(y_te, proba)\n",
    "    ll  = log_loss(y_te, proba)\n",
    "\n",
    "    fold_results.append({'fold': fold+1, 'acc': acc, 'auc': auc, 'logloss': ll})\n",
    "    print(f'  {fold+1:<6} {acc:<12.3f} {auc:<10.3f} {ll:.3f}')\n",
    "    final_model = model  # save last fold for SHAP\n",
    "\n",
    "res = pd.DataFrame(fold_results)\n",
    "print(f'\\n  Mean Accuracy : {res[\"acc\"].mean():.3f} Â± {res[\"acc\"].std():.3f}')\n",
    "print(f'  Mean AUC      : {res[\"auc\"].mean():.3f} Â± {res[\"auc\"].std():.3f}')\n",
    "\n",
    "df_model['lgbm_proba'] = all_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Feature Importance â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Train final model on full data\n",
    "full_model = lgb.LGBMClassifier(**lgbm_params)\n",
    "full_model.fit(X, y_bin, callbacks=[lgb.log_evaluation(-1)])\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature':    ALL_FEATURES,\n",
    "    'importance': full_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, max(6, len(ALL_FEATURES)*0.3)))\n",
    "top_n = importance_df.head(25)\n",
    "\n",
    "colors = []\n",
    "for feat in top_n['feature']:\n",
    "    if feat in ['rate_regime', 'vix_regime', 'tnx', 'vix', 'tnx_1m_chg', 'vix_5d_chg']:\n",
    "        colors.append('#F44336')   # macro = red\n",
    "    elif feat in ['sue', 'eps_surprise_pct', 'hist_beat_rate', 'beat_streak']:\n",
    "        colors.append('#4CAF50')   # earnings = green\n",
    "    elif 'ret_' in feat or 'rsi' in feat or 'ma' in feat:\n",
    "        colors.append('#2196F3')   # technical = blue\n",
    "    else:\n",
    "        colors.append('#9C27B0')   # other = purple\n",
    "\n",
    "ax.barh(top_n['feature'], top_n['importance'], color=colors, edgecolor='white')\n",
    "ax.invert_yaxis()\n",
    "ax.set_xlabel('Feature Importance (LightGBM Gain)')\n",
    "ax.set_title('Feature Importance\\nðŸ”´ Macro  ðŸŸ¢ Earnings Surprise  ðŸ”µ Technical  ðŸŸ£ Other')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/04_feature_importance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Key question: does rate_regime appear in top 10?\n",
    "rate_rank = importance_df[importance_df['feature'] == 'rate_regime'].index\n",
    "if len(rate_rank) > 0:\n",
    "    print(f'\\nrate_regime rank: #{importance_df.index.get_loc(rate_rank[0])+1} of {len(importance_df)}')\n",
    "    print('â†’ If in top 15, structural break theory is validated by the model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ SHAP Analysis â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# SHAP shows *why* each prediction was made â€” not just overall importance\n",
    "\n",
    "print('Computing SHAP values (this takes ~1 minute)...')\n",
    "explainer   = shap.TreeExplainer(full_model)\n",
    "shap_values = explainer.shap_values(X)\n",
    "\n",
    "# If binary, shap_values may be a list [class0, class1]\n",
    "sv = shap_values[1] if isinstance(shap_values, list) else shap_values\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "plt.sca(axes[0])\n",
    "shap.summary_plot(\n",
    "    sv, X, feature_names=ALL_FEATURES,\n",
    "    max_display=15, show=False, plot_type='bar'\n",
    ")\n",
    "axes[0].set_title('Mean |SHAP| â€” Overall Feature Importance')\n",
    "\n",
    "plt.sca(axes[1])\n",
    "shap.summary_plot(\n",
    "    sv, X, feature_names=ALL_FEATURES,\n",
    "    max_display=15, show=False\n",
    ")\n",
    "axes[1].set_title('SHAP Beeswarm â€” Feature Direction & Magnitude')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/04_shap_summary.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('\\nHow to read the beeswarm:')\n",
    "print('  Right of 0 = increases predicted outperform probability')\n",
    "print('  Red dots = high feature value, Blue = low feature value')\n",
    "print('  e.g. Red dots on right for SUE â†’ high SUE increases P(outperform)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Regime Interaction: Do Feature Importances Change? â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Train separate models for Low Rate vs High Rate periods\n",
    "# Compare which features matter in each regime\n",
    "\n",
    "low_rate_idx  = np.where(df_model['rate_regime'].values == 0)[0]\n",
    "high_rate_idx = np.where(df_model['rate_regime'].values == 1)[0]\n",
    "\n",
    "regime_importances = {}\n",
    "\n",
    "for name, idx in [('Low Rate (2019-21)', low_rate_idx),\n",
    "                   ('High Rate (2022+)', high_rate_idx)]:\n",
    "    if len(idx) < 100:\n",
    "        print(f'Skipping {name}: only {len(idx)} samples')\n",
    "        continue\n",
    "\n",
    "    X_reg = X[idx]\n",
    "    y_reg = y_bin[idx]\n",
    "\n",
    "    m = lgb.LGBMClassifier(**lgbm_params)\n",
    "    m.fit(X_reg, y_reg, callbacks=[lgb.log_evaluation(-1)])\n",
    "\n",
    "    regime_importances[name] = pd.Series(\n",
    "        m.feature_importances_, index=ALL_FEATURES\n",
    "    ).sort_values(ascending=False)\n",
    "    print(f'{name}: {len(idx)} events')\n",
    "\n",
    "# Plot comparison\n",
    "if len(regime_importances) == 2:\n",
    "    top_features = list(regime_importances.values())[0].head(15).index\n",
    "    comp = pd.DataFrame(regime_importances)[top_features].T\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(11, 6))\n",
    "    x = np.arange(len(top_features))\n",
    "    width = 0.38\n",
    "\n",
    "    ax.bar(x - width/2, comp.iloc[0], width, label=comp.index[0],\n",
    "           color='#2196F3', alpha=0.85, edgecolor='white')\n",
    "    ax.bar(x + width/2, comp.iloc[1], width, label=comp.index[1],\n",
    "           color='#F44336', alpha=0.85, edgecolor='white')\n",
    "\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(top_features, rotation=45, ha='right', fontsize=9)\n",
    "    ax.set_ylabel('Feature Importance')\n",
    "    ax.set_title('Feature Importance by Rate Regime\\n(Testing Structural Break Theory)')\n",
    "    ax.legend(fontsize=10)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../outputs/04_regime_importance.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print('\\nIf bars differ significantly between regimes â†’ structural break is real')\n",
    "    print('e.g. Revenue growth more important in low-rate era; margins more important in high-rate era')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ 3-Class Model: Buy / Hold / Sell â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print('Training 3-class magnitude model...')\n",
    "\n",
    "# 0=Significant Miss, 1=Neutral, 2=Significant Beat\n",
    "tscv3 = TimeSeriesSplit(n_splits=5, gap=5)\n",
    "all_pred_mag = np.full(len(y_mag), -1, dtype=int)\n",
    "\n",
    "lgbm_multi = lgb.LGBMClassifier(\n",
    "    **{**lgbm_params, 'class_weight': None, 'num_class': 3}\n",
    ")\n",
    "\n",
    "fold_accs = []\n",
    "for fold, (tr, te) in enumerate(tscv3.split(X)):\n",
    "    m = lgb.LGBMClassifier(**{**lgbm_params,\n",
    "                               'objective': 'multiclass',\n",
    "                               'num_class': 3,\n",
    "                               'class_weight': None})\n",
    "    m.fit(X[tr], y_mag[tr], callbacks=[lgb.log_evaluation(-1)])\n",
    "    preds = m.predict(X[te])\n",
    "    all_pred_mag[te] = preds\n",
    "    fold_accs.append(accuracy_score(y_mag[te], preds))\n",
    "\n",
    "print(f'\\n3-Class Accuracy: {np.mean(fold_accs):.3f} Â± {np.std(fold_accs):.3f}')\n",
    "print(f'Naive baseline (always predict Neutral): {(y_mag == 1).mean():.3f}')\n",
    "\n",
    "# Confusion matrix\n",
    "valid_mask = all_pred_mag >= 0\n",
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "ConfusionMatrixDisplay.from_predictions(\n",
    "    y_mag[valid_mask], all_pred_mag[valid_mask],\n",
    "    display_labels=['Sig. Miss', 'Neutral', 'Sig. Beat'],\n",
    "    cmap='Blues', ax=ax\n",
    ")\n",
    "ax.set_title('3-Class Confusion Matrix\\n(diagonal = correct predictions)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/04_confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Save predictions\n",
    "df_model['lgbm_pred_mag'] = all_pred_mag\n",
    "df_model[['ticker', 'earn_date', 'target_binary', 'target_mag',\n",
    "           'excess_ret_5d', 'lgbm_proba', 'lgbm_pred_mag',\n",
    "           'rate_regime', 'vix_regime']].to_csv(\n",
    "    '../data/lgbm_predictions.csv', index=False\n",
    ")\n",
    "print('\\nâœ“ Predictions saved to data/lgbm_predictions.csv')"
   ]
  }
 ],
 "metadata": {\n",
  \"kernelspec\": {\"display_name\": \"Python 3\", \"language\": \"python\", \"name\": \"python3\"},\n",
  \"language_info\": {\"name\": \"python\", \"version\": \"3.11.0\"}\n",
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
