{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 — Logistic Regression Baseline\n",
    "\n",
    "**Goal:** Prove the concept with a simple, interpretable model before moving to Random Forest.\n",
    "Logistic Regression gives us clean coefficients — we can see *exactly* what drives predictions.\n",
    "\n",
    "**Features used:**\n",
    "- `sue` — Standardized Unexpected Earnings\n",
    "- `ret_14d` — Pre-earnings 14-day momentum\n",
    "- `rate_regime` — High/Low rate environment\n",
    "- `vix_regime` — Fear/Complacency environment\n",
    "- `hist_beat_rate` — Historical beat frequency\n",
    "\n",
    "**Evaluation:** TimeSeriesSplit (never K-Fold — that's lookahead), CAR plots on predicted winners/losers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, roc_auc_score, log_loss,\n",
    "    classification_report, ConfusionMatrixDisplay\n",
    ")\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "panel = pd.read_csv('../data/panel_dataset.csv', parse_dates=['earn_date'])\n",
    "panel = panel.sort_values('earn_date').reset_index(drop=True)\n",
    "print(f'Panel loaded: {panel.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Feature selection for Logistic Regression ─────────────────\n",
    "# Keep it minimal and interpretable for the MVP\n",
    "LOGIT_FEATURES = [\n",
    "    # Earnings surprise\n",
    "    'sue',\n",
    "    'eps_surprise_pct',\n",
    "    'hist_beat_rate',\n",
    "    'beat_streak',\n",
    "\n",
    "    # Pre-earnings momentum\n",
    "    'ret_5d',\n",
    "    'ret_14d',\n",
    "    'ret_20d',\n",
    "\n",
    "    # Volatility context\n",
    "    'rvol_pctile',\n",
    "    'vol_term_ratio',\n",
    "\n",
    "    # Volume validation\n",
    "    'vol_ratio_5d',\n",
    "    'vol_ratio_1d',\n",
    "\n",
    "    # Macro regime\n",
    "    'rate_regime',\n",
    "    'vix_regime',\n",
    "    'tnx',\n",
    "    'vix',\n",
    "    'vix_5d_chg',\n",
    "\n",
    "    # Ticker history\n",
    "    'ticker_avg_abs_move',\n",
    "    'ticker_up_rate',\n",
    "]\n",
    "\n",
    "# Keep features that exist in the panel\n",
    "LOGIT_FEATURES = [f for f in LOGIT_FEATURES if f in panel.columns]\n",
    "print(f'Features used: {len(LOGIT_FEATURES)}')\n",
    "print(LOGIT_FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Prepare X, y ──────────────────────────────────────────────\n",
    "df_model = panel[LOGIT_FEATURES + ['target_binary', 'earn_date',\n",
    "                                    'excess_ret_5d', 'ticker']].copy()\n",
    "df_model = df_model.dropna(subset=['target_binary', 'sue'])\n",
    "df_model = df_model.sort_values('earn_date').reset_index(drop=True)\n",
    "\n",
    "X = df_model[LOGIT_FEATURES].values\n",
    "y = df_model['target_binary'].values\n",
    "\n",
    "print(f'Modelling dataset: {X.shape[0]} events, {X.shape[1]} features')\n",
    "print(f'Class balance: {y.mean():.1%} outperform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── TimeSeriesSplit Cross-Validation ──────────────────────────\n",
    "# IMPORTANT: TimeSeriesSplit ensures we always train on past, test on future.\n",
    "# Random K-Fold would leak future data into training — a critical error.\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5, gap=5)  # gap=5 avoids near-boundary leakage\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),  # handle missing values\n",
    "    ('scaler',  StandardScaler()),                  # normalise features\n",
    "    ('model',   LogisticRegression(\n",
    "        C=0.5,                  # L2 regularisation (lower = stronger)\n",
    "        class_weight='balanced',  # corrects class imbalance\n",
    "        max_iter=500,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "fold_results = []\n",
    "all_proba    = np.zeros(len(y))\n",
    "all_pred     = np.zeros(len(y))\n",
    "\n",
    "print('Running TimeSeriesSplit cross-validation...\\n')\n",
    "print(f'  {\"Fold\":<6} {\"Train N\":<10} {\"Test N\":<10} {\"Accuracy\":<12} {\"AUC\":<10} {\"LogLoss\"}')\n",
    "print('  ' + '-'*58)\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(tscv.split(X)):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "    pred  = pipeline.predict(X_test)\n",
    "\n",
    "    all_proba[test_idx] = proba\n",
    "    all_pred[test_idx]  = pred\n",
    "\n",
    "    acc  = accuracy_score(y_test, pred)\n",
    "    auc  = roc_auc_score(y_test, proba)\n",
    "    ll   = log_loss(y_test, proba)\n",
    "\n",
    "    fold_results.append({'fold': fold+1, 'train_n': len(train_idx),\n",
    "                         'test_n': len(test_idx), 'accuracy': acc,\n",
    "                         'auc': auc, 'logloss': ll})\n",
    "    print(f'  {fold+1:<6} {len(train_idx):<10} {len(test_idx):<10} '\n",
    "          f'{acc:<12.3f} {auc:<10.3f} {ll:.3f}')\n",
    "\n",
    "results_df = pd.DataFrame(fold_results)\n",
    "print('\\n  Averages:')\n",
    "print(f'  Accuracy : {results_df[\"accuracy\"].mean():.3f} ± {results_df[\"accuracy\"].std():.3f}')\n",
    "print(f'  AUC      : {results_df[\"auc\"].mean():.3f} ± {results_df[\"auc\"].std():.3f}')\n",
    "print(f'  LogLoss  : {results_df[\"logloss\"].mean():.3f} ± {results_df[\"logloss\"].std():.3f}')\n",
    "print()\n",
    "print('Benchmark: Naive \"always predict outperform\" = 50.0% accuracy, AUC = 0.500')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Calibration Curve ─────────────────────────────────────────\n",
    "# Are predicted probabilities reliable enough for position sizing?\n",
    "\n",
    "test_mask = all_proba > 0  # only rows that got predictions\n",
    "prob_true, prob_pred = calibration_curve(\n",
    "    y[test_mask], all_proba[test_mask], n_bins=10\n",
    ")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
    "\n",
    "# Calibration\n",
    "axes[0].plot(prob_pred, prob_true, 'o-', color='steelblue', linewidth=2, label='Model')\n",
    "axes[0].plot([0, 1], [0, 1], '--', color='gray', linewidth=1, label='Perfect calibration')\n",
    "axes[0].fill_between([0,1], [0,0], [1,1], alpha=0.05, color='gray')\n",
    "axes[0].set_xlabel('Predicted Probability')\n",
    "axes[0].set_ylabel('Actual Fraction Positive')\n",
    "axes[0].set_title('Calibration Curve\\n(closer to diagonal = more reliable probabilities)')\n",
    "axes[0].legend()\n",
    "\n",
    "# Probability distribution\n",
    "axes[1].hist(all_proba[test_mask & (y == 1)], bins=30, alpha=0.6,\n",
    "             color='green', label='Actual Outperform', density=True)\n",
    "axes[1].hist(all_proba[test_mask & (y == 0)], bins=30, alpha=0.6,\n",
    "             color='red', label='Actual Underperform', density=True)\n",
    "axes[1].axvline(0.5, color='black', linestyle='--')\n",
    "axes[1].set_xlabel('Predicted Probability of Outperform')\n",
    "axes[1].set_ylabel('Density')\n",
    "axes[1].set_title('Predicted Probability Distribution\\n(separation = discriminative power)')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/03_calibration.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Coefficient Plot ──────────────────────────────────────────\n",
    "# Train final model on all data to see coefficients\n",
    "pipeline.fit(X, y)\n",
    "coefs = pipeline.named_steps['model'].coef_[0]\n",
    "\n",
    "coef_df = pd.DataFrame({\n",
    "    'feature': LOGIT_FEATURES,\n",
    "    'coef':    coefs\n",
    "}).sort_values('coef', key=abs, ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, max(5, len(coef_df)*0.35)))\n",
    "colors = ['#2196F3' if c > 0 else '#F44336' for c in coef_df['coef']]\n",
    "ax.barh(coef_df['feature'], coef_df['coef'], color=colors, edgecolor='white')\n",
    "ax.axvline(0, color='black', linewidth=0.8)\n",
    "ax.set_xlabel('Logistic Regression Coefficient')\n",
    "ax.set_title('Logistic Regression Coefficients\\n(Blue = positive effect on outperform probability)')\n",
    "ax.invert_yaxis()\n",
    "\n",
    "# Annotation\n",
    "ax.text(0.99, 0.01, 'Blue → increases P(outperform)\\nRed → decreases P(outperform)',\n",
    "        transform=ax.transAxes, ha='right', va='bottom', fontsize=8,\n",
    "        bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/03_coefficients.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Key check: Is SUE coefficient positive? (It should be — higher surprise = higher outperform chance)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Top-Decile Precision ──────────────────────────────────────\n",
    "# Most tradeable signal: when the model is very confident, is it right?\n",
    "\n",
    "df_model['proba']    = all_proba\n",
    "df_model['pred']     = all_pred\n",
    "df_model['in_test']  = (all_proba > 0).astype(int)\n",
    "\n",
    "df_test = df_model[df_model['in_test'] == 1].copy()\n",
    "df_test['confidence_decile'] = pd.qcut(\n",
    "    df_test['proba'].rank(method='first'), q=10, labels=False\n",
    ") + 1\n",
    "\n",
    "decile_stats = df_test.groupby('confidence_decile').agg(\n",
    "    actual_outperform_rate=('target_binary', 'mean'),\n",
    "    avg_excess_ret=('excess_ret_5d', 'mean'),\n",
    "    n_events=('target_binary', 'count')\n",
    ").reset_index()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
    "\n",
    "axes[0].bar(decile_stats['confidence_decile'],\n",
    "            decile_stats['actual_outperform_rate'] * 100,\n",
    "            color='steelblue', edgecolor='white')\n",
    "axes[0].axhline(50, color='red', linestyle='--', label='50% baseline')\n",
    "axes[0].set_xlabel('Confidence Decile (1=least, 10=most confident)')\n",
    "axes[0].set_ylabel('Actual Outperform Rate (%)')\n",
    "axes[0].set_title('Precision by Confidence Decile\\n(should be monotonically increasing)')\n",
    "axes[0].legend()\n",
    "axes[0].set_ylim(0, 100)\n",
    "\n",
    "axes[1].bar(decile_stats['confidence_decile'],\n",
    "            decile_stats['avg_excess_ret'] * 100,\n",
    "            color=['tomato' if v < 0 else 'steelblue'\n",
    "                   for v in decile_stats['avg_excess_ret']],\n",
    "            edgecolor='white')\n",
    "axes[1].axhline(0, color='black', linewidth=0.8)\n",
    "axes[1].set_xlabel('Confidence Decile')\n",
    "axes[1].set_ylabel('Avg Excess Return vs S&P 500 (%)')\n",
    "axes[1].set_title('Average Return by Confidence Decile')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/03_decile_precision.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "top_decile = decile_stats[decile_stats['confidence_decile'] == 10].iloc[0]\n",
    "print(f'\\nTop decile: {top_decile[\"actual_outperform_rate\"]:.1%} outperform rate, '\n",
    "      f'{top_decile[\"avg_excess_ret\"]*100:+.2f}% avg excess return')\n",
    "print('Target: >60% outperform rate in top decile for a tradeable signal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Save model probabilities for backtest notebook ─────────────\n",
    "df_model[['ticker', 'earn_date', 'target_binary', 'excess_ret_5d',\n",
    "          'proba', 'pred', 'rate_regime', 'vix_regime']].to_csv(\n",
    "    '../data/logit_predictions.csv', index=False\n",
    ")\n",
    "print('✓ Predictions saved to data/logit_predictions.csv')\n",
    "print('  → These feed into the backtest in notebook 05')"
   ]
  }
 ],
 "metadata": {\n",
  \"kernelspec\": {\"display_name\": \"Python 3\", \"language\": \"python\", \"name\": \"python3\"},\n",
  \"language_info\": {\"name\": \"python\", \"version\": \"3.11.0\"}\n",
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
